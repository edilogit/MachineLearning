{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,mean_absolute_error\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto Final Statistical Learning\n",
    "\n",
    "### Parte 1: Entrenamiento, Selección y Validación\n",
    "\n",
    "#### En esta parte se carga el dataset y se hace split en tres partes las cuales son entrenamiento, validacion y test. Solo se usara por el momento entrenamiento y validacion. Luego se hace una seleccion de variables mediante feature eng. y  se crean los 4 modelos con sus respectivos hiperparametros. Por ultimo se hace una validacion con las metricas de evaluacion de cada modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\titanic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga y split de datos\n",
    "delimitador = ','\n",
    "df = pd.read_csv('data_titanic_proyecto.csv',delimitador)\n",
    "\n",
    "X = df[['PassengerId','Name','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked','passenger_class','passenger_sex']]\n",
    "y = df['passenger_survived']\n",
    "\n",
    "#divido datos para train y test\n",
    "x_trainTemp, x_test, y_trainTemp, y_test = train_test_split(X, y, test_size=0.20, random_state=15)\n",
    "\n",
    "#vuelvo a dividir datos de trainTemp para train y validacion\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trainTemp, y_trainTemp, test_size=0.20, random_state=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>445</td>\n",
       "      <td>Johannesen-Bratthammer, Mr. Bernt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65306</td>\n",
       "      <td>8.1125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>764</td>\n",
       "      <td>Carter, Mrs. William Ernest (Lucile Polk)</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113760</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>466</td>\n",
       "      <td>Goncalves, Mr. Manuel Estanslas</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101306</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>389</td>\n",
       "      <td>Sadlier, Mr. Matthew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>367655</td>\n",
       "      <td>7.7292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>Givard, Mr. Hans Kristensen</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250646</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                                       Name   Age  SibSp  \\\n",
       "444          445          Johannesen-Bratthammer, Mr. Bernt   NaN      0   \n",
       "763          764  Carter, Mrs. William Ernest (Lucile Polk)  36.0      1   \n",
       "465          466            Goncalves, Mr. Manuel Estanslas  38.0      0   \n",
       "388          389                       Sadlier, Mr. Matthew   NaN      0   \n",
       "213          214                Givard, Mr. Hans Kristensen  30.0      0   \n",
       "\n",
       "     Parch              Ticket      Fare    Cabin Embarked passenger_class  \\\n",
       "444      0               65306    8.1125      NaN        S           Lower   \n",
       "763      2              113760  120.0000  B96 B98        S           Upper   \n",
       "465      0  SOTON/O.Q. 3101306    7.0500      NaN        S           Lower   \n",
       "388      0              367655    7.7292      NaN        Q           Lower   \n",
       "213      0              250646   13.0000      NaN        S          Middle   \n",
       "\n",
       "    passenger_sex  \n",
       "444             M  \n",
       "763             F  \n",
       "465             M  \n",
       "388             M  \n",
       "213             M  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>311</td>\n",
       "      <td>Hays, Miss. Margaret Bechstein</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11767</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C54</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>636</td>\n",
       "      <td>Davis, Miss. Mary</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237668</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>Devaney, Miss. Margaret Delia</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330958</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>699</td>\n",
       "      <td>Thayer, Mr. John Borland</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17421</td>\n",
       "      <td>110.8833</td>\n",
       "      <td>C68</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>751</td>\n",
       "      <td>Wells, Miss. Joan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29103</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                            Name   Age  SibSp  Parch  Ticket  \\\n",
       "310          311  Hays, Miss. Margaret Bechstein  24.0      0      0   11767   \n",
       "635          636               Davis, Miss. Mary  28.0      0      0  237668   \n",
       "44            45   Devaney, Miss. Margaret Delia  19.0      0      0  330958   \n",
       "698          699        Thayer, Mr. John Borland  49.0      1      1   17421   \n",
       "750          751               Wells, Miss. Joan   4.0      1      1   29103   \n",
       "\n",
       "         Fare Cabin Embarked passenger_class passenger_sex  \n",
       "310   83.1583   C54        C           Upper             F  \n",
       "635   13.0000   NaN        S          Middle             F  \n",
       "44     7.8792   NaN        Q           Lower             F  \n",
       "698  110.8833   C68        C           Upper             M  \n",
       "750   23.0000   NaN        S          Middle             F  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>216</td>\n",
       "      <td>Newell, Miss. Madeleine</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35273</td>\n",
       "      <td>113.2750</td>\n",
       "      <td>D36</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>464</td>\n",
       "      <td>Milling, Mr. Jacob Christian</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>234360</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>422</td>\n",
       "      <td>Charters, Mr. David</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 13032</td>\n",
       "      <td>7.7333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>275</td>\n",
       "      <td>Healy, Miss. Hanora \"Nora\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370375</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                            Name   Age  SibSp  Parch  \\\n",
       "215          216         Newell, Miss. Madeleine  31.0      1      0   \n",
       "463          464    Milling, Mr. Jacob Christian  48.0      0      0   \n",
       "27            28  Fortune, Mr. Charles Alexander  19.0      3      2   \n",
       "421          422             Charters, Mr. David  21.0      0      0   \n",
       "274          275      Healy, Miss. Hanora \"Nora\"   NaN      0      0   \n",
       "\n",
       "         Ticket      Fare        Cabin Embarked passenger_class passenger_sex  \n",
       "215       35273  113.2750          D36        C           Upper             F  \n",
       "463      234360   13.0000          NaN        S          Middle             M  \n",
       "27        19950  263.0000  C23 C25 C27        S           Upper             M  \n",
       "421  A/5. 13032    7.7333          NaN        Q           Lower             M  \n",
       "274      370375    7.7500          NaN        Q           Lower             F  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 11)\n",
      "(143, 11)\n",
      "(179, 11)\n",
      "(569,)\n",
      "(143,)\n",
      "(179,)\n"
     ]
    }
   ],
   "source": [
    "#sizing datasets\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_val))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_val))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding y Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215    1\n",
       "463    0\n",
       "27     0\n",
       "421    0\n",
       "274    1\n",
       "dtype: int8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#haciendo label enconding de la variable Y  survived=1  died=2\n",
    "y_train=y_train.astype('category')\n",
    "y_train= y_train.cat.codes\n",
    "y_train.head()\n",
    "\n",
    "y_test=y_test.astype('category')\n",
    "y_test= y_test.cat.codes\n",
    "y_test.head()\n",
    "\n",
    "y_val=y_val.astype('category')\n",
    "y_val= y_val.cat.codes\n",
    "y_val.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##borramos las columnas que consideramos no serviran porque son categoricas o tienen muchos valores null\n",
    "x_train.drop('Name', axis=1, inplace=True)\n",
    "x_test.drop('Name', axis=1, inplace=True)\n",
    "x_val.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "x_train.drop('Cabin', axis=1, inplace=True)\n",
    "x_test.drop('Cabin', axis=1, inplace=True)\n",
    "x_val.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "x_train.drop('Ticket', axis=1, inplace=True)\n",
    "x_test.drop('Ticket', axis=1, inplace=True)\n",
    "x_val.drop('Ticket', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.drop('PassengerId', axis=1, inplace=True)\n",
    "#x_test.drop('PassengerId', axis=1, inplace=True)\n",
    "#x_val.drop('PassengerId', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#llenamos con 0 la variable Age cuando no esta especificada\n",
    "x_train.isnull().sum()\n",
    "x_train[\"Age\"]= x_train[\"Age\"].fillna('0')\n",
    "\n",
    "x_test.isnull().sum()\n",
    "x_test[\"Age\"]= x_test[\"Age\"].fillna('0')\n",
    "\n",
    "x_val.isnull().sum()\n",
    "x_val[\"Age\"]= x_val[\"Age\"].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos label encoding para las variables Passenger_sex, passenger_class y Embarked\n",
    "le = preprocessing.LabelEncoder()\n",
    "x_train['passenger_sex'] = le.fit_transform(x_train['passenger_sex'])\n",
    "x_train['passenger_class'] = le.fit_transform(x_train['passenger_class'])\n",
    "x_train[\"Embarked\"] = le.fit_transform(x_train[\"Embarked\"].fillna('0'))\n",
    "x_train['Embarked'] = le.fit_transform(x_train['Embarked'])\n",
    "\n",
    "\n",
    "x_test['passenger_sex'] = le.fit_transform(x_test['passenger_sex'])\n",
    "x_test['passenger_class'] = le.fit_transform(x_test['passenger_class'])\n",
    "x_test[\"Embarked\"] = le.fit_transform(x_test[\"Embarked\"].fillna('0'))\n",
    "x_test['Embarked'] = le.fit_transform(x_test['Embarked'])\n",
    "\n",
    "\n",
    "x_val['passenger_sex'] = le.fit_transform(x_val['passenger_sex'])\n",
    "x_val['passenger_class'] = le.fit_transform(x_val['passenger_class'])\n",
    "x_val[\"Embarked\"] = le.fit_transform(x_val[\"Embarked\"].fillna('0'))\n",
    "x_val['Embarked'] = le.fit_transform(x_val['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1125</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>764</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>466</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>389</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7292</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId Age  SibSp  Parch      Fare  Embarked  passenger_class  \\\n",
       "444          445   0      0      0    8.1125         3                0   \n",
       "763          764  36      1      2  120.0000         3                2   \n",
       "465          466  38      0      0    7.0500         3                0   \n",
       "388          389   0      0      0    7.7292         2                0   \n",
       "213          214  30      0      0   13.0000         3                1   \n",
       "\n",
       "     passenger_sex  \n",
       "444              1  \n",
       "763              0  \n",
       "465              1  \n",
       "388              1  \n",
       "213              1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "#### Si se hubiera requerido hacer bootstraping se logra generando muchos set de datos a partir del proporcionado, esto mediante un metodo llamado sampling, el dataset se replica con o sin reemplazo en los valores. Sobre estos dataset de muestra se calcula el estadistico que necesitamos para obtener un aproximado del estadistico poblacional, un pseudocodigo de bootstrap se encuentra a continuacion\n",
    "\n",
    "for i in bootstraps:\n",
    "\n",
    "\tsample = select_sample_with_replacement(data)\n",
    "    \n",
    "\tstat = calculate_statistic(sample)\n",
    "    \n",
    "\tstatistics.append(stat)\n",
    "\n",
    "#### El proceso bootstrap se puede usar para evaluar el rendimiento de un algoritmo de machine learning, como cada muestra se hace con el 60% u 80% de los datos, hay datos que nunca pueden ser obtenidos en la muestra, estos se denominan OOB del ingles Out of Bag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ensemble_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELOS\n",
    "\n",
    "## Arbol de Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scoring = 'accuracy'\n",
    "#score = cross_val_score(mod, x_train, y_train, cv=10, n_jobs=1, scoring=scoring)\n",
    "#print(score)\n",
    "#print('Media Decision Tree:', round(np.mean(score)*100, 2))\n",
    "\n",
    "def train_DecisionTree(X,Y,presrt,split,file):\n",
    "    decision_tree = DecisionTreeClassifier(presort=presrt,splitter=split) \n",
    "    decision_tree.fit(x_train, y_train)  \n",
    "    y_pred = decision_tree.predict(x_test)      \n",
    "\n",
    "#model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "    filename = file\n",
    "    joblib.dump(decision_tree, filename)\n",
    "    \n",
    "    accuracy = round(decision_tree.score(x_train, y_train) * 100, 2)\n",
    "    f_1_score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    prec_score = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec_score = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    Mean_ab_error=mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return accuracy,f_1_score,prec_score,rec_score,Mean_ab_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod = SVC()\n",
    "#scoring = 'accuracy'\n",
    "#score = cross_val_score(mod, x_train, y_train, cv=10, n_jobs=1, scoring=scoring)\n",
    "#print(score)\n",
    "#print('Media SVM:', round(np.mean(score)*100, 2))\n",
    "\n",
    "def train_SVM(X,Y,penalty,kern,deg,file):\n",
    "    linear_svc = SVC(C=penalty,kernel=kern)\n",
    "    linear_svc.fit(x_train, y_train)\n",
    "    y_pred = linear_svc.predict(x_test)    \n",
    "\n",
    "    filename = file\n",
    "    joblib.dump(linear_svc, filename)\n",
    "\n",
    "    acc_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\n",
    "    f_1_score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    prec_score = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec_score = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    Mean_ab_error=mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return acc_svc,f_1_score,prec_score,rec_score,Mean_ab_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod = GaussianNB()\n",
    "#scoring = 'accuracy'\n",
    "#score = cross_val_score(mod, x_train, y_train, cv=10, n_jobs=1, scoring=scoring)\n",
    "#print(score)\n",
    "#print('Media SVM:', round(np.mean(score)*100, 2))\n",
    "\n",
    "def train_NB(X,Y,file):\n",
    "    gaussian = GaussianNB() \n",
    "    gaussian.fit(x_train, y_train)  \n",
    "    y_pred = gaussian.predict(x_test)  \n",
    "\n",
    "    filename = file\n",
    "    joblib.dump(gaussian, filename)\n",
    "    acc_NB = round(gaussian.score(x_train, y_train) * 100, 2)\n",
    "    f_1_score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    prec_score = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    rec_score = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    Mean_ab_error=mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    return acc_NB,f_1_score,prec_score,rec_score,Mean_ab_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Logistica con Regularizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train_test_Reg_Log(session,predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False,file=\"\"):\n",
    "\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "        # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [cost_op,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in np.arange(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in np.arange(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {x: Xd.iloc[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            if training is not None:\n",
    "                W = np.asarray(session.run(variables,feed_dict))\n",
    "                W.dump(file)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        \n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\envs\\envgalileo.Python\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numFeatures = x_train.shape[1]\n",
    "numLabels = 2\n",
    "\n",
    "    # clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "Lambda = 0.01 #Regularization Parameter\n",
    "learningRate = 1\n",
    "    \n",
    "\n",
    "# Logistic Regression\n",
    "def Titanicmodel(x,y,is_training):   \n",
    "    weights=tf.get_variable(\"weights\",shape=[numFeatures,numLabels])\n",
    "    bias=tf.get_variable(\"bias\",shape=[numLabels])\n",
    "    y_out = tf.matmul(x,weights)+bias\n",
    "    return(y_out,weights)\n",
    "\n",
    "\n",
    "y_out,weights = Titanicmodel(x,y,is_training)\n",
    "    \n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.one_hot(y,2),logits=y_out))\n",
    "regularizer = tf.nn.l2_loss(weights)\n",
    "cost_op = tf.reduce_mean(loss + Lambda * regularizer)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learningRate)\n",
    "train_step = optimizer.minimize(cost_op)\n",
    "\n",
    "    #Prediction\n",
    "prediction = tf.argmax(y_out,1)\n",
    "    #Lets strat a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "#saver.save(sess, 'regLog_lr=0.01_reg=0.1_var1_var2_var3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 59.1 and accuracy of 0.29\n",
      "Epoch 1, Overall loss = 106 and accuracy of 0.351\n",
      "Epoch 2, Overall loss = 94.6 and accuracy of 0.355\n",
      "Epoch 3, Overall loss = 89.9 and accuracy of 0.374\n",
      "Epoch 4, Overall loss = 100 and accuracy of 0.306\n",
      "Epoch 5, Overall loss = 101 and accuracy of 0.33\n",
      "Epoch 6, Overall loss = 98.1 and accuracy of 0.327\n",
      "Epoch 7, Overall loss = 110 and accuracy of 0.304\n",
      "Epoch 8, Overall loss = 122 and accuracy of 0.336\n",
      "Epoch 9, Overall loss = 104 and accuracy of 0.36\n",
      "Epoch 10, Overall loss = 101 and accuracy of 0.362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.8911056116302, 0.36203866432337434)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training')\n",
    "train_test_Reg_Log(sess,y_out,cost_op,x_train,y_train,10,100,100,train_step,False,file=\"test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 36.6 and accuracy of 0.0699\n",
      "Epoch 2, Overall loss = 36.6 and accuracy of 0.0699\n",
      "Epoch 3, Overall loss = 36.6 and accuracy of 0.0699\n",
      "Epoch 4, Overall loss = 36.6 and accuracy of 0.0699\n",
      "Epoch 5, Overall loss = 36.6 and accuracy of 0.0699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36.593211274047, 0.06993006993006994)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Validation')\n",
    "train_test_Reg_Log(sess,y_out,cost_op,x_val,y_val,5,25,25, file=\"test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTACION\n",
    "### En esta seccion se corren todos los procesos y se anota en un excel que luego se cargara en este notebook manualmente  para cada corrida de cada modelo se guardara el job correspondiente y se anotaran las metricas para tabularlas y mostrarlas en este notebook en las siguiente secciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "Decision Trees (100.0, 0.7193908564481556, 0.7198076923076924, 0.7190214614205417, 0.2737430167597765)\n",
      "Decision Trees (100.0, 0.7413377861982594, 0.7429568363918325, 0.7401635155850792, 0.25139664804469275)\n",
      "Decision Trees (100.0, 0.7001675041876047, 0.6994854417670682, 0.7030531425651507, 0.29608938547486036)\n",
      "Decision Trees (100.0, 0.7620886075949367, 0.7608750314307267, 0.765074092999489, 0.2346368715083799)\n"
     ]
    }
   ],
   "source": [
    "#Distintos parametros Decision Trees\n",
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "print('Decision Trees' , train_DecisionTree(x_train, y_train,False,'best',file=\"DecTree_sort_false_splitter_best_train\"))\n",
    "print('Decision Trees' , train_DecisionTree(x_train, y_train,True,'best',file=\"DecTree_sort_true_splitter_best_train\"))\n",
    "print('Decision Trees' , train_DecisionTree(x_train, y_train,False,'random',file=\"DecTree_sort_false_splitter_random_train\"))\n",
    "print('Decision Trees' , train_DecisionTree(x_train, y_train,True,'random',file=\"DecTree_sort_true_splitter_random_train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "SVM (99.82, 0.3795350556916393, 0.7893258426966292, 0.506578947368421, 0.41899441340782123)\n",
      "SVM (100.0, 0.3795350556916393, 0.7893258426966292, 0.506578947368421, 0.41899441340782123)\n",
      "SVM (62.92, 0.36524822695035464, 0.2877094972067039, 0.5, 0.4245810055865922)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "print('SVM',train_SVM(x_train, y_train,1,'rbf',1,file=\"SVM_Penalty_1_kernel_rbf_degree_1_train\"))\n",
    "print('SVM',train_SVM(x_train, y_train,2,'rbf',1,file=\"SVM_Penalty_2_kernel_rbf_degree_1_train\"))\n",
    "print('SVM',train_SVM(x_train, y_train,1,'sigmoid',1,file=\"SVM_Penalty_1_kernel_sigmoid_degree_1_train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "Naive-Bayes (78.38, 0.8224717663243434, 0.823076923076923, 0.8219213081246806, 0.17318435754189945)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "print('Naive-Bayes',train_NB(x_train, y_train,file=\"NBayes_lr=0.01_reg=0.1_train\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "Iteration 0: with minibatch training loss = 205 and accuracy of 0.36\n",
      "Epoch 1, Overall loss = 115 and accuracy of 0.38\n",
      "Epoch 2, Overall loss = 87.7 and accuracy of 0.332\n",
      "Epoch 3, Overall loss = 118 and accuracy of 0.341\n",
      "Epoch 4, Overall loss = 90.7 and accuracy of 0.334\n",
      "Epoch 5, Overall loss = 115 and accuracy of 0.385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(115.28579260050098, 0.38488576449912126)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "train_test_Reg_Log(sess,y_out,cost_op,x_train,y_train,5,100,100,train_step,False,file=\"regLog_lr=0.01_reg=0.001_train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "Iteration 0: with minibatch training loss = 113 and accuracy of 0.28\n",
      "Epoch 1, Overall loss = 98.4 and accuracy of 0.359\n",
      "Epoch 2, Overall loss = 105 and accuracy of 0.35\n",
      "Epoch 3, Overall loss = 107 and accuracy of 0.343\n",
      "Epoch 4, Overall loss = 95.2 and accuracy of 0.334\n",
      "Epoch 5, Overall loss = 114 and accuracy of 0.385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(113.83808224481942, 0.38488576449912126)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "train_test_Reg_Log(sess,y_out,cost_op,x_train,y_train,5,100,100,train_step,False,file=\"regLog_lr=0.02_reg=0.1_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "Iteration 0: with minibatch training loss = 121 and accuracy of 0.28\n",
      "Epoch 1, Overall loss = 89.7 and accuracy of 0.362\n",
      "Epoch 2, Overall loss = 101 and accuracy of 0.33\n",
      "Epoch 3, Overall loss = 96.5 and accuracy of 0.364\n",
      "Epoch 4, Overall loss = 113 and accuracy of 0.383\n",
      "Epoch 5, Overall loss = 100 and accuracy of 0.357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.150464612696, 0.35676625659050965)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "train_test_Reg_Log(sess,y_out,cost_op,x_train,y_train,5,100,100,train_step,False,file=\"regLog_lr=0.5_reg=0.01_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy f1_score precision_score recall_score Mean_ab_error\n",
      "Iteration 0: with minibatch training loss = 153 and accuracy of 0.28\n",
      "Epoch 1, Overall loss = 104 and accuracy of 0.351\n",
      "Epoch 2, Overall loss = 109 and accuracy of 0.339\n",
      "Epoch 3, Overall loss = 92.1 and accuracy of 0.334\n",
      "Epoch 4, Overall loss = 111 and accuracy of 0.381\n",
      "Epoch 5, Overall loss = 100 and accuracy of 0.359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.421998250254, 0.3585237258347979)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy','f1_score','precision_score','recall_score','Mean_ab_error')\n",
    "train_test_Reg_Log(sess,y_out,cost_op,x_train,y_train,5,100,100,train_step,False,file=\"regLog_lr=1.0_reg=0.01_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnica KFold\n",
    "#### En la validación cruzada de K iteraciones o K-fold cross-validation los datos se dividen en K subconjuntos (folds). Uno de los subconjuntos se utiliza como datos de prueba y el resto (K-1) como datos de entrenamiento.  El proceso de validación cruzada es repetido durante K iteraciones, con cada uno de los posibles subconjuntos de datos de prueba.\n",
    "\n",
    "#### Como se hubiera usado K-fold en el proyecto? primero se define un numero K para hacer los folds, en este caso puede ser 5, luego se toma el data set y se entrena con los K-1 folds, repetir hasta agotar el k-esimo elemento y promediar los resultados para obtener un solo valor de performance que servira de metrica para nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\KFold-cross_validation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experimento</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>Mean_ab_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecTree_sort_false_splitter_best_train</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.719391</td>\n",
       "      <td>0.719808</td>\n",
       "      <td>0.719021</td>\n",
       "      <td>0.273743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecTree_sort_true_splitter_best_train</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.741338</td>\n",
       "      <td>0.742957</td>\n",
       "      <td>0.740164</td>\n",
       "      <td>0.251397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecTree_sort_false_splitter_random_train</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.700168</td>\n",
       "      <td>0.699485</td>\n",
       "      <td>0.703053</td>\n",
       "      <td>0.296089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecTree_sort_true_splitter_random_train</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.762089</td>\n",
       "      <td>0.760875</td>\n",
       "      <td>0.765074</td>\n",
       "      <td>0.234637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM_Penalty_1_kernel_rbf_degree_1_train</td>\n",
       "      <td>99.820000</td>\n",
       "      <td>0.379535</td>\n",
       "      <td>0.789326</td>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.418994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM_Penalty_1_kernel_rbf_degree_1_train</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.379535</td>\n",
       "      <td>0.789326</td>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.418994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM_Penalty_1_kernel_sigmoid_degree_1_train</td>\n",
       "      <td>62.920000</td>\n",
       "      <td>0.365248</td>\n",
       "      <td>0.287709</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.424581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NBayes_lr=0.01_reg=0.1_train</td>\n",
       "      <td>78.380000</td>\n",
       "      <td>0.822472</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.821921</td>\n",
       "      <td>0.173184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>regLog_lr=0.01_reg=0.001_train</td>\n",
       "      <td>38.488576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>regLog_lr=0.02_reg=0.1_train</td>\n",
       "      <td>38.488576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>regLog_lr=0.5_reg=0.01_train</td>\n",
       "      <td>35.676626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>regLog_lr=1.0_reg=0.01_train</td>\n",
       "      <td>35.852373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Experimento    Accuracy  f1_score  \\\n",
       "0        DecTree_sort_false_splitter_best_train  100.000000  0.719391   \n",
       "1         DecTree_sort_true_splitter_best_train  100.000000  0.741338   \n",
       "2      DecTree_sort_false_splitter_random_train  100.000000  0.700168   \n",
       "3       DecTree_sort_true_splitter_random_train  100.000000  0.762089   \n",
       "4       SVM_Penalty_1_kernel_rbf_degree_1_train   99.820000  0.379535   \n",
       "5       SVM_Penalty_1_kernel_rbf_degree_1_train  100.000000  0.379535   \n",
       "6   SVM_Penalty_1_kernel_sigmoid_degree_1_train   62.920000  0.365248   \n",
       "7                  NBayes_lr=0.01_reg=0.1_train   78.380000  0.822472   \n",
       "8                regLog_lr=0.01_reg=0.001_train   38.488576  0.000000   \n",
       "9                  regLog_lr=0.02_reg=0.1_train   38.488576  0.000000   \n",
       "10                 regLog_lr=0.5_reg=0.01_train   35.676626  0.000000   \n",
       "11                 regLog_lr=1.0_reg=0.01_train   35.852373  0.000000   \n",
       "\n",
       "    precision_score  recall_score  Mean_ab_error  \n",
       "0          0.719808      0.719021       0.273743  \n",
       "1          0.742957      0.740164       0.251397  \n",
       "2          0.699485      0.703053       0.296089  \n",
       "3          0.760875      0.765074       0.234637  \n",
       "4          0.789326      0.506579       0.418994  \n",
       "5          0.789326      0.506579       0.418994  \n",
       "6          0.287709      0.500000       0.424581  \n",
       "7          0.823077      0.821921       0.173184  \n",
       "8          0.000000      0.000000       0.115286  \n",
       "9          0.000000      0.000000       0.113838  \n",
       "10         0.000000      0.000000       0.100150  \n",
       "11         0.000000      0.000000       0.100422  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se cargan los datos de experimentos capturados en cada prueba anterior\n",
    "data = pd.read_csv(\"experimentos.csv\") \n",
    "\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONES DE EXPERIMENTACION\n",
    "\n",
    "### En esta etapa se hicieron varios modelos y se ejecutaron en los datasets de entrenamiento, posteriormente se aplicaron en datos de validacion y se obtuvieron las metricas siguientes Accuracy, F1, Precision, Recall y Mean_Errors.  Durante todo este proceso me doy cuenta que hay modelos mas rapidos que otros y modelos que tienen mejores metricas que otros, sin embargo, no hay ninguno que sea mejor en todas las metricas. En mi caso el modelo que mejor se ha desempeñado es el de Naive-Bayes en el cual se logro un nivel de exactitud mayor al 80%. La dificultad mayor enfrentada en este proyecto fue hacer funcionar el modelo de regresion lineal en tensorflow con regularizacion.  Otra dificultad fue probar distintos tipos de kernel para el modelo de SVM ya que al probar polinomial (poly) y lineal (linear) se quedaba ejecutando sin responder durante muchos minutos, por lo cual solamente se probo con el valor default  rbf y sigmoid, siendo el mejor el kernel default (rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
